1.全闪分布式
1.1 架构的特征：
  1）异步event模型——Thread模型的优势是代码结构简单，编码生产效率高，但是性能开销大。在业务逻辑简单，性能要求高的系统中，彻底抛弃thread和mutex是必然选择。1ht1线程，polling无锁队列，run-to-completion。
  2）用户态驱动——用户态tcp，减少用户空间到kernel空间的上下文切换，同时也减少内存拷贝。
  3）RDMA & NVMeoF——100GRDMA RoCE或者IB接口，以及NVMeoF，已经成了高性能分布式存储的标准配置。但是配置了100G IB/RoCE 也无法提升性能，社区版Ceph就存在这个问题，说明软件栈才是瓶颈
  4）高性能本地存储——直接采用ext4、xfs等单机文件系统作为本地存储模块，早期的 Ceph也是这么做的，使用了十年之后，Ceph研发团队终于意识到这并不是最佳选择。

1.2 存储介质
ZNS——ZNS接口声称可以减少SSD的OP成本，减少GC导致的性能开销，我并不认同。用append-only接口替换掉write-in-place接口，并没有真正省掉OP成本和GC开销，只是把这些成本和开销从SSD盘内部转移到了应用层。
     使用ZNS接口应用，必须自己设计类似LSM Tree的数据结构，自己定期对LSM Tree做Compaction以回收无用空间。
     那些没有被即时回收的空间，跟SSD盘内部配置的OP是等价的。从应用发起的Compaction流程，占用了HOST的CPU处理能力，并且占用了HOST到SSD的总线带宽。
     ZNS的真正潜力在于，允许SSD盘配置性能更弱的CPU，以及允许SSD盘配置更少的DRAM，从而让SSD盘的成本下降。
     可是，硬件成本不只跟复杂度相关，跟销量本身相关性更强，销量越大成本越低。现在看来，ZNS靠简化SSD盘内部设计来节约掉的成本，并不足以抵消普通NVMe接口SSD由于销量巨大而带来的优势。
     重新为SSD盘定义块接口，已经有过很多尝试了。但是，只有NVMe获得了巨大成功。Open Channel算是失败了。NVMe中有一个Streams选项，也没有得到广泛应用。
     现在看来， ZNS也很难成功。最近两年又有人提出FDP，是另外一个版本的Streams，FDP是否能得到推广，尚未可知。
SCM——随着Intel Optane PM宣布停产，字节级寻址的持久化内存的发展速度慢了下来。无论 如何SCM技术不会消失，很可能在CXL时代大放异彩。
     现在开发的新架构，必须考虑如何 利用好SCM。如果力量足够强，应该投入一些研发资源持续跟进SCM技术的发展。
HDD——早有预测说，2027年SSD的每TB存储成本会下降到与HDD持平，到时候HDD将彻底失去 生存空间，HDD会像BP机、MP3、傻瓜相机、车载GPS导航仪一样，彻底消失。
     在我看来， 也许2027年太早，HDD还不至于彻底消失。这个时间点，也许是2030年，或者2035年， 总之，这个时间点一定会到来，不需要等太久。
     在2023年还继续把研发成本投入 到SSD和HDD的混合存储产品上，是不明智的。

1.3 重删（重复数据删除？？）
    全闪阵列支持重删，背后的逻辑是：SSD盘的随机读性能，在全闪阵列中用不完的，富裕很多。CPU性能也是有富裕的，那么，就把这些富裕的SSD和CPU的性能利用起来，用在重删上面，来降低每TB成本。
    主存储的重删，不能只有产品支持，还要配合商业模式，也就是，不再按照SSD盘的物理容量定价，而是按照客户存入的数据量定价。数据缩减率低的风险，厂商承担。这是全闪阵列成功的原因。
    现在单片SSD的性能就可以达到百万IOPS。另外一方面，有十几个服务器节点，上百个SSD盘的Ceph集群，也是百万IOPS。
    现在搞全闪阵列，或者全闪分布式，是否还应该搞重删，值得重新思考审视。
    在我看来，把CPU的每一条指令，都用到提升IOPS和带宽上，是更明智的选择。

1.4 开源软件
    现在讨论分布式存储，已经很难避开Ceph、DAOS、SPDK这些开源软件。Ceph已然垂垂老矣，但凡有一点实力的Ceph厂商，已经开始着手开发下一代架构的产品，或者基于 DAOS， 或者全自研的新架构。
    在Optane停产之后，Intel是否会停止对DAOS的投入，是一个值得关注的风险点。
    SPDK并不是简单包装一下就可以上市销售的完整系统，只是一些零碎的存储相关驱动和协议的工具模块库。
    SPDK里面的工具模块性能和质量都还可以，只是必须全流程按照SPDK设定的轮询模式才能充分发挥出它的性能。

1.5 Share Everything架构
    share everything架构这个口号是vastdata喊出来的，Vastdata是2023增长最迅猛的 存储初创公司。存储行业的研发人员，都不应该忽视这个公司，以及它的DASE架构。
    share everything架构分为两层：1）下面的存储层是双控的EBOF全闪存储硬盘框，2）上面业务层运行在标准服务器节点上的软件集群。
    所有业务节点通过100G RDMA网络共享访问EBOF存储节点。
    EBOF存储节点的特点是双控、双口nvme SSD盘、每个控制器带有多个100G RDMA网口。
    Vastdata自己并不生产EBOF硬盘框，委托其他厂商生产。
    vastdata的目标是让EBOF硬盘框变成像标准服务器一样的廉价硬件，这样围绕share everything 架构生态系统才能发展起来。
    这种架构可以在集群规模不大的情况下支持大比例EC，这样可以降低每TB存储成本。
    与基于标准服务器的分布式SDS相比，这种架构把最小故障域从服务器节点缩小到了SSD盘，一个EBOF框中单个控制器故障，
    还有另外一个控制可以工作，不会导致整个EBOF框故障离线，这种故障模型跟双控阵列一样。
    虽然最近20年网络技术发展很快，网络带宽迅速从千兆发展到100G，但是IB和RoCE网络的带宽成本都比服务器内部PCIe总线的带宽成本高很多。
    至少现在，跨越100G IB/RoCE访问EBOF硬盘框，并没有性价比优势。

2.数据库底座
