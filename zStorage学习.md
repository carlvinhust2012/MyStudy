1.全闪分布式
1.1 架构的特征：
  1）异步event模型——Thread模型的优势是代码结构简单，编码生产效率高，但是性能开销大。在业务逻辑简单，性能要求高的系统中，彻底抛弃thread和mutex是必然选择。1ht1线程，polling无锁队列，run-to-completion。
  2）用户态驱动——用户态tcp，减少用户空间到kernel空间的上下文切换，同时也减少内存拷贝。
  3）RDMA & NVMeoF——100GRDMA RoCE或者IB接口，以及NVMeoF，已经成了高性能分布式存储的标准配置。但是配置了100G IB/RoCE 也无法提升性能，社区版Ceph就存在这个问题，说明软件栈才是瓶颈
  4）高性能本地存储——直接采用ext4、xfs等单机文件系统作为本地存储模块，早期的 Ceph也是这么做的，使用了十年之后，Ceph研发团队终于意识到这并不是最佳选择。

1.2 存储介质
ZNS——ZNS接口声称可以减少SSD的OP成本，减少GC导致的性能开销，我并不认同。用append-only接口替换掉write-in-place接口，并没有真正省掉OP成本和GC开销，只是把这些成本和开销从SSD盘内部转移到了应用层。
     使用ZNS接口应用，必须自己设计类似LSM Tree的数据结构，自己定期对LSM Tree做Compaction以回收无用空间。
     那些没有被即时回收的空间，跟SSD盘内部配置的OP是等价的。从应用发起的Compaction流程，占用了HOST的CPU处理能力，并且占用了HOST到SSD的总线带宽。
     ZNS的真正潜力在于，允许SSD盘配置性能更弱的CPU，以及允许SSD盘配置更少的DRAM，从而让SSD盘的成本下降。
     可是，硬件成本不只跟复杂度相关，跟销量本身相关性更强，销量越大成本越低。现在看来，ZNS靠简化SSD盘内部设计来节约掉的成本，并不足以抵消普通NVMe接口SSD由于销量巨大而带来的优势。
     重新为SSD盘定义块接口，已经有过很多尝试了。但是，只有NVMe获得了巨大成功。Open Channel算是失败了。NVMe中有一个Streams选项，也没有得到广泛应用。
     现在看来， ZNS也很难成功。最近两年又有人提出FDP，是另外一个版本的Streams，FDP是否能得到推广，尚未可知。
SCM——随着Intel Optane PM宣布停产，字节级寻址的持久化内存的发展速度慢了下来。无论 如何SCM技术不会消失，很可能在CXL时代大放异彩。
     现在开发的新架构，必须考虑如何 利用好SCM。如果力量足够强，应该投入一些研发资源持续跟进SCM技术的发展。
HDD——早有预测说，2027年SSD的每TB存储成本会下降到与HDD持平，到时候HDD将彻底失去 生存空间，HDD会像BP机、MP3、傻瓜相机、车载GPS导航仪一样，彻底消失。
     在我看来， 也许2027年太早，HDD还不至于彻底消失。这个时间点，也许是2030年，或者2035年， 总之，这个时间点一定会到来，不需要等太久。
     在2023年还继续把研发成本投入 到SSD和HDD的混合存储产品上，是不明智的。

1.3 重删（重复数据删除？？）
    全闪阵列支持重删，背后的逻辑是：SSD盘的随机读性能，在全闪阵列中用不完的，富裕很多。CPU性能也是有富裕的，那么，就把这些富裕的SSD和CPU的性能利用起来，用在重删上面，来降低每TB成本。
    主存储的重删，不能只有产品支持，还要配合商业模式，也就是，不再按照SSD盘的物理容量定价，而是按照客户存入的数据量定价。数据缩减率低的风险，厂商承担。这是全闪阵列成功的原因。
    现在单片SSD的性能就可以达到百万IOPS。另外一方面，有十几个服务器节点，上百个SSD盘的Ceph集群，也是百万IOPS。
    现在搞全闪阵列，或者全闪分布式，是否还应该搞重删，值得重新思考审视。
    在我看来，把CPU的每一条指令，都用到提升IOPS和带宽上，是更明智的选择。

1.4 开源软件
    现在讨论分布式存储，已经很难避开Ceph、DAOS、SPDK这些开源软件。Ceph已然垂垂老矣，但凡有一点实力的Ceph厂商，已经开始着手开发下一代架构的产品，或者基于 DAOS， 或者全自研的新架构。
    在Optane停产之后，Intel是否会停止对DAOS的投入，是一个值得关注的风险点。
    SPDK并不是简单包装一下就可以上市销售的完整系统，只是一些零碎的存储相关驱动和协议的工具模块库。
    SPDK里面的工具模块性能和质量都还可以，只是必须全流程按照SPDK设定的轮询模式才能充分发挥出它的性能。

1.5 Share Everything架构
    share everything架构这个口号是vastdata喊出来的，Vastdata是2023增长最迅猛的 存储初创公司。存储行业的研发人员，都不应该忽视这个公司，以及它的DASE架构。
    share everything架构分为两层：1）下面的存储层是双控的EBOF全闪存储硬盘框，2）上面业务层运行在标准服务器节点上的软件集群。
    所有业务节点通过100G RDMA网络共享访问EBOF存储节点。EBOF存储节点的特点是双控、双口nvme SSD盘、每个控制器带有多个100G RDMA网口。
    Vastdata自己并不生产EBOF硬盘框，委托其他厂商生产。
    vastdata的目标是让EBOF硬盘框变成像标准服务器一样的廉价硬件，这样围绕share everything 架构生态系统才能发展起来。
    这种架构可以在集群规模不大的情况下支持大比例EC，这样可以降低每TB存储成本。
    与基于标准服务器的分布式SDS相比，这种架构把最小故障域从服务器节点缩小到了SSD盘，一个EBOF框中单个控制器故障，
    还有另外一个控制可以工作，不会导致整个EBOF框故障离线，这种故障模型跟双控阵列一样。
    虽然最近20年网络技术发展很快，网络带宽迅速从千兆发展到100G，但是IB和RoCE网络的带宽成本都比服务器内部PCIe总线的带宽成本高很多。
    至少现在，跨越100G IB/RoCE访问EBOF硬盘框，并没有性价比优势。
1.6 共识协议
    CAP理论：一致性+可用性+分区容错
    CAP——分布式系统的设计难点是故障处理。如何在某些部件发生故障的情况，保持系统正常工作状态，包括，数据不丢失，并且能够正确处理从主机所收到的IO请求。这就是故障处理流程要考虑的问题。
    在分布式系统中，没有永远不发生故障部件，所有部件都可能发生故障。
    对关键问题的判断，需要采用”投票”的方式，共同做出决策。投票，就需要有一个规则，大家按照这个规则来投票。这个投票的规则，就是Paxos和Raft这类共识协议。
    一般来说，新的分布式系统没必要再定义自己特殊的“投票规则”了，直接采用Paxos和Raft就好了，这两个共识协议，很多双同行的眼睛仔细检视过了，经过了TLA+这种形式化语言的证明。
    因此，对分布式存储系统的设计者来说，透彻理解Paxos和Raft很重要。

    分布式存储软件的架构，由控制面和数据面两个部分来构成。系统运行的关键决策由控制面负责，例如，集群由哪些节点构成，每个节点负责处理哪部分数据，哪个节点发生了故障等等。
    控制面由一些控制节点构成，控制节点按照共识协议的规则来共同决策，这保证了决策的连贯和一致，避免了“集群脑裂”。
    数据面负责处理IO请求，也需要对一些问题做决策。例如，何时可以给主机回复写成功？数据有三个副本，应该从哪个副本读？等等，如果这些问题全部交给控制面决策，那么控制面就会变成瓶颈，集群的性能会很差。
    数据面需要自行对某些问题做决策，因此，数据面也需要运行共识协议。

    Multi-Raft
    1）需要对多个Raft组的心跳消息进行合并。zStorage在数据面使用Multi-raft，把一个存储池的数据打散(sharding)到多个Raft组中，每个节点和硬盘承载多个不同Raft组的副本，在扩容或者硬件故障情况下，
    在节点和盘之间重新分布这些Raft副本，以此达成可弹性伸缩的目标。每个节点上有多个Raft组，心跳消息就会很多，会占用很多网络带宽和CPU处理能力，实际上，并不需要在每个Raft组都维持自己的心跳，
    只需要在节点之间维持心跳就 可以探测到节点（或者进程）故障了。
  
    2）考虑对多个Raft组的日志合并之后写盘。日志合并之后， 减少了对SSD盘随机写的次数。另一方面，SSD盘写操作的最小单位是512Bytes的数据块，如果不做合并，要等到Raft日志凑齐一个512B，可能需要很长时间。
    如果不等凑齐就写盘，那么SSD盘上就会有比较多的空洞，增加了写放大，影响性能，也影响 SSD盘的寿命。
    zStorage是块存储系统，不涉及跨越多个Raft组的分布式事务管理，这方面比分布式数据库简单一些。
    
    3）面临的问题：
    Raft论文中讲了两种实现成员变更的方法：
    1、单步骤只变更一个成员，也就是在 一次变更中，要么增加一个成员，要么减少一个成员，不能直接替换掉一个成员。
       例如，要实现由(A, B, C)==>(A, B, D)的变更必须分为两步走，第一步先做 (A, B, C)==>(A, B, C, D)，第二步再做(A, B, C, D)==>(A, B, D)。这种方法 比较简单，但是有可用性问题；
    2、第二种做法是joint consensus方法，实现稍复杂，没有可用性问题。很多开源的Raft都没有实现joint consensus方法。

    硬盘状态机
    zStorage是一个分布式存储系统，对它来说，集群所有服务器节点硬盘上的数据全集， 构成了Raft状态机。但是，Raft有一个隐含的基本假设是，状态机是在内存中的，并且尺寸并不很大。
    虽然Raft的大论文中有一节提到了硬盘状态机，但是篇幅并不长，只给了一些基本的处理原则，并没有具体的设计。
    Erasure Code
    实际上，Raft并不需要每个成员都保持一个完整的状态机，一个Raft组中所有成员加 起来有一份完整的状态机就够了。
    在分布式存储产品上实现基于Raft的EC，目前可供参考的例子并不多，很多问题需要自行解决，zStorage也正在探索中。
    
2.数据库底座

