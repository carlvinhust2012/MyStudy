初步的理解，kvcache相当于一个分布式内存池
2025 年主流路线有两条，都直接把 KVCache 做成「分布式内存池」：
1）云厂商托管版（阿里云 Tair KVCache、火山引擎 EIC），参考：https://www.modb.pro/db/1943491310539780096
已商用，显存-内存-SSD 三级池化，单卡可 offload 百 GB 级 KVCache，TTFT 缩短 90%，Batch 规模提升 5-10 倍 。
2）开源社区版（Mooncake + SGLang/MemServe）
2025-07 代码合入主干，用 RDMA/GPUDirect 把集群空闲 GPU 显存+CPU 内存拼成统一池，支持 PD 分离、多副本、哈希去重，单机 10 分钟能搭，多机可水平扩展到 TB 级 。
因此，无论是买服务还是自己搭，「KVCache 分布式内存池」都已从论文变成生产线标配，不再是“会不会”，而是“用哪一家”。

分布式内存池，里面的内存，都是跨机器的，访问速度会不会比本机内存要慢？
——会，但慢得很少，而且通常被 TTFT/Batch 提升的收益完全抵消；是否可接受取决于你用的网络硬件和 chunk 调度策略。
结论：跨机比本机显存慢 8–12 倍，但仍在 1 µs 级；相比一次 Attention 计算（几十 µs）和 Python 框架调度（几百 µs）只是小头。

| 访问路径                        | 典型延迟           | 相对本机显存倍数   |
| --------------------------     | --------------    | ---------- |
| 本机 HBM（GPU 显存）             | 100–150 ns      | 1×         |
| 本机 DRAM（CPU 内存）            | 80–100 ns       | ≈1×        |
| **RDMA read**（100 Gbps，跨机） | **1.2–1.5 µs**   | **10–12×** |
| **RoCEv2/200 Gbps**           | **0.8–1.0 µs**   | **8×**     |
| NVLink + NVSwitch（单机 8 卡）  | 200–300 ns       | 2×         |

二、为什么“感觉不明显”
1）流水线掩盖
Mooncake/MemServe 把 KV-Block 切成 512 KB–1 MB 的 chunk，预取 + 双缓冲：当前 token 做计算时，下一个 chunk 已提前拉到本地显存，网络延迟被完全覆盖。
2）批量传输
一次跨机读 64–128 MB，有效带宽 90 % 以上；平均到每个 token 只增加 0.02–0.04 ms。
3）PD 分离架构
prefill 节点本来就要算完才送 decode 节点，1 µs 网络延迟 ≪ 几十 ms 计算延迟，对首 token 时间几乎没影响
